{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Нейронные сети и компьютерное зрение`\n",
    "\n",
    "[Презентации GitHub](https://github.com/NazarovMichail/NN-CV-presentations)\n",
    "\n",
    "[Ноутбуки GitHub](https://github.com/NazarovMichail/NN-CV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Нейрон и нейронная сеть}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a> Схема нейрона</a>\n",
    "\n",
    "<img src='img/neuron.png'>\n",
    "\n",
    "- `Разделяющая поверхность` - это геометрическое место точек, где функция активации меняет свое значение \n",
    "\n",
    "- Разделяющая поверхность задается формулой :\n",
    "\n",
    "    - $<\\vec{x}, \\vec{w}> + b = 0$\n",
    "\n",
    "        - Вектор $\\vec{w}$ является нормалью к гиперплоскости, которая задана этим уравнением\n",
    "\n",
    "<img src='img/surface.png' width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Функция активации</a>\n",
    "\n",
    "`Функция активации` - `дифференцируемая` функция, применяемая к линейному преобразованию для создания `нелинейного` преобразования признаков и аппроксимации `нелинейных зависимостей` в данных\n",
    "\n",
    "<img src='img/func.png' width=700>\n",
    "\n",
    "- `Пороговая` функция активации всегда будет равна 1 с той стороны от разделяющей прямой, в которую указывает вектор $\\vec{w}$\n",
    "\n",
    "`Производная` функции активации :\n",
    "\n",
    "- `Сигмоида` \n",
    "    - $\\boxed { \\sigma(\\vec{w}) = \\frac{1}{1 - e^{-\\vec{w}}} }$ : \n",
    "\n",
    "        - $\\boxed{\\frac{\\delta \\sigma(\\vec{w})}{\\delta \\vec{w} } = \\sigma(\\vec{w}) \\cdot (1 - \\sigma(\\vec{w})) }$\n",
    "\n",
    "- `Гиперболический тангенс` \n",
    "    - $\\boxed{ th(\\vec{w}) = \\frac{e^{\\vec{w} } - e^{- \\vec{ w} }}  {e^{\\vec{w} } + e^{-\\vec{w} }  } }$ :\n",
    "\n",
    "        - $\\boxed{\\frac{\\delta th(\\vec{w})}{\\delta \\vec{w} } = 1 - th(\\vec{w}) ^2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Правило цепочки</a>\n",
    "\n",
    "`Правило дифференцирования` сложной функции :\n",
    "\n",
    "<img src='img/rule.png'>\n",
    "\n",
    "Для функции потерь :\n",
    "\n",
    "$L\\Bigg( f_4\\bigg(f_2(f_1(x)), f_3(f_1(x))\\bigg) \\Bigg) $\n",
    "\n",
    "`Граф` вычисления :\n",
    "\n",
    "<img src='img/chain.png'>\n",
    "\n",
    "`Производная функции потерь` по параметрам модели :\n",
    "\n",
    "$\\boxed{ \\frac{\\delta L}{\\delta w_1} = \\frac{\\delta L}{\\delta y_4} \\cdot \\bigg[ \\frac{\\delta f_4}{\\delta y_2} \\cdot \\frac{\\delta f_2}{\\delta y_1} + \\frac{\\delta f_4}{\\delta y_3} \\cdot \\frac{\\delta f_3}{\\delta y_1}\\bigg] \\cdot \\frac{\\delta f_1}{\\delta w_1} }$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "#### <a style='color:grey'>Пример (правило цепочки)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "- Записать математическое выражение, которое задает граф :\n",
    "\n",
    "    - $x_i$ : входы в граф\n",
    "\n",
    "    - $b_i, c_i$ : настраиваемые параметры\n",
    "\n",
    "<img src='img/forw_t1.png' width=400>\n",
    "\n",
    "$\\boxed{ y = ((x_1+b_1) * \\sigma(x_2+b_2))*c_1 + \\sigma(x_1+b_1) * \\tanh(x_2+b_2) * c_2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "- Найти производную $y$ по параметру $c_2$ :\n",
    "​\n",
    " \n",
    "<img src='img/back_t1.png' width=400>\n",
    "\n",
    "$\\boxed{ \\frac{\\delta y}{\\delta c_2} = \\frac{\\delta y}{\\delta z_9} \\cdot \\frac{\\delta z_9}{\\delta z_6} \\cdot \\frac{\\delta z_6}{\\delta c_2} }$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "- Найти производную $y$ по параметру $b_1$ :\n",
    "​\n",
    " \n",
    "<img src='img/back_t1.png' width=400>\n",
    "\n",
    "$\\boxed{ \\frac{\\delta y}{\\delta b_1} = (\\frac{\\delta y}{\\delta z_9} \\cdot \\frac{\\delta z_9}{\\delta z_3} \\cdot\\frac{\\delta z_3}{\\delta z_1} + \\frac{\\delta y}{\\delta z_8} \\cdot \\frac{\\delta z_8}{\\delta z_7} \\cdot\\frac{\\delta z_7}{\\delta z_1}) \\cdot \\frac{\\delta z_1}{\\delta b_1} } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "- Найти производную $y$ по параметру $b_2$ :\n",
    "​\n",
    " \n",
    "<img src='img/back_t1.png' width=400>\n",
    "\n",
    "$\\boxed{ \\frac{\\delta y}{\\delta b_2} = (\\frac{\\delta y}{\\delta z_8} \\cdot \\frac{\\delta z_8}{\\delta z_7} \\cdot\\frac{\\delta z_7}{\\delta z_4} \\cdot\\frac{\\delta z_4}{\\delta z_2} + \\frac{\\delta y}{\\delta z_9} \\cdot \\frac{\\delta z_9}{\\delta z_6} \\cdot\\frac{\\delta z_6}{\\delta z_5} \\cdot\\frac{\\delta z_5}{\\delta z_2}) \\cdot \\frac{\\delta z_2}{\\delta b_2} } $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "- Найти производную $y$ по  $c_2$, выраженную через `входные параметры` $x_i, b_i$ :\n",
    " \n",
    "<img src='img/back_t1.png' width=400>\n",
    "\n",
    "$\\boxed{ \\frac{\\delta y}{\\delta c_2} =  \\frac{\\delta y}{\\delta z_9}\\cdot \\frac{\\delta z_9}{\\delta z_6} \\cdot \\frac{\\delta z_6}{\\delta c_2}} $ :\n",
    "\n",
    "$\\boxed{1}$ \n",
    "- $\\boxed{ \\frac{\\delta y}{\\delta z_9}} = \\frac{\\delta (z_9 + z_8)}{\\delta z_9} = \\boxed {1}$\n",
    "\n",
    "$\\boxed{2}$ \n",
    "- $\\boxed{ \\frac{\\delta z_9}{\\delta z_6}} = \\frac{\\delta (z_3 \\cdot z_6)}{\\delta z_6} = z_3$\n",
    "\n",
    "    - $z_3 = \\sigma(z_1) = \\boxed{\\sigma(x_1 + b_1)}$\n",
    "\n",
    "\n",
    "$\\boxed{3}$ \n",
    "\n",
    "- $\\boxed{ \\frac{\\delta z_6}{\\delta c_2}} = \\frac{\\delta (z_5 \\cdot z_2)}{\\delta z_2} = z_5$\n",
    "\n",
    "    - $z_5 = \\tanh(z_2) =  \\boxed{\\tanh(x_2 + b_2)}$\n",
    "\n",
    "$\\boxed{ \\frac{\\delta y}{\\delta c_2} = 1 \\cdot \\sigma(x_1 + b_1) \\cdot  \\tanh(x_2 + b_2)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Задачи ИНС}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Бинарная классификация</a>\n",
    "\n",
    "`Функция потерь: Бинарная кросс-энтропия`\n",
    "\n",
    "$\\boxed{BCE(y, \\sigma(x)) = - y \\cdot \\ln \\sigma(x) - \\big(1- y\\big) \\cdot \\big(1 - \\ln \\sigma(x)\\big) }$\n",
    "- $\\sigma(x)$ : выход сети, интерпретируемый как вероятность класса\n",
    "- $y$: действительный класс объекта\n",
    "\n",
    "`Градиент`\n",
    "\n",
    "$\\frac{\\delta BCE(y, \\sigma(x))}{\\delta x } = \\frac{\\delta BCE(y, \\sigma(x))}{\\sigma(x) } \\cdot  \\frac{\\sigma(x)}{x } $\n",
    "\n",
    "- $\\boxed{\\frac{\\delta BCE(y, \\sigma(x))}{\\delta x } = \\sigma(x) - y }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Мультиклассовая классификация</a>\n",
    "\n",
    "`Функция активации: Софтмакс`\n",
    "\n",
    "$\\boxed{SM(y)_i =\\frac{e^{y_i}}{\\sum_{j=1}^n e^{y_j}}}$\n",
    "\n",
    "- $y_i$: выход сети, как любое действительное число\n",
    "- $\\sum_{j=1}^n e^{y_j}$: сумма всех экспонент выходов сети, как вероятности классов\n",
    "\n",
    "`Градиент`\n",
    "\n",
    "$\\boxed{\\frac{\\delta SM(y_i)}{\\delta y_c } = - SM(y_i) \\cdot SM(y_c) }$\n",
    "\n",
    "$\\boxed{\\frac{\\delta SM(y_i)}{\\delta y_i } =  SM(y_i) \\cdot \\big(1 - SM(y_i)\\big) }$\n",
    "\n",
    "- $y_c$: выход не целевого объекта, как любое действительное число\n",
    "- $y_i$: выход целевого объекта, как любое действительное число\n",
    "\n",
    "`Функция потерь: Кросс-энтропия`\n",
    "\n",
    "$\\boxed{CE(t, SM) = - \\sum_{c=1}^n t_c \\cdot \\ln SM_c }$\n",
    "\n",
    "- $t_c$: таргетное значение для класса\n",
    "- $SM_c$: софтмакс для выходов сети\n",
    "\n",
    "`Градиент`\n",
    "\n",
    "$\\boxed{\\frac{\\delta CE}{\\delta y_i } = - t_i +  SM_i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Детекция</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/detect.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Центр объекта`\n",
    "\n",
    "$X_c \\in [0, 1 ]$, что является `областью значений сигмоиды`\n",
    "- 0 : левый нижний угол всего изображения\n",
    "- 1 : правый нижниый угол всего изображения\n",
    "\n",
    "$Y_c \\in [0, 1 ]$, что является `областью значений сигмоиды`\n",
    "- 0 : левый нижний угол всего изображения\n",
    "- 1 : левый верхний угол всего изображения\n",
    "\n",
    "`Высота и ширина объекта`\n",
    "\n",
    "$w \\in [0, +\\infty ]$: , что является `областью значений экспоненты`\n",
    "\n",
    "$h \\in [0, +\\infty ]$: , что является `областью значений экспоненты`\n",
    "\n",
    "`Функция потерь`\n",
    "\n",
    "- $\\boxed{BCE( \\sigma (z_0), I) }$: бинарная кросс-энтропия для выходов вероятностей классов \n",
    "    - $I$: индикатор, есть ли целевой объект на изображении\n",
    "- $\\boxed{ I \\cdot \\Big(BCE( \\sigma (z_1), x_c) + BCE( \\sigma (z_2), y_c) \\Big) }$: бинарная кросс-энтропия для выходов центра объекта\n",
    "\n",
    "- $\\boxed{MSE(z_3, \\ln w) }$, т.к.\n",
    "    - $\\hat{w} = e^{z_3}$: ширина выданная сетью\n",
    "    - $t_3 = \\ln w$: теоретический выход, необходимый, для того, чтобы получить таргет-ширину\n",
    "        - $MSE(z_3, t_3)$, сравниваются теоретический и практический выходы сети, вне зависимости абсолютного значения ширины, во сколько раз практическая ширина меньше или больше теоретической"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Сегментация</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/segment.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Теоретическое` значение пикселей \n",
    "- Значение пикселя относящегося к объекту: 1\n",
    "- Значение пикселя не относящегося к объекту: 0\n",
    "\n",
    "`Практическое` значение пикселей \n",
    "- Вероятность того, что пиксель относится к объекту $\\in [0, 1 ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Сжатие размерности</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/autoenc.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Кодировщик` преобразует изображение в сжатое представление (эмбеддинг)\n",
    "    - Кол-во параметров сильно меньше, чем в исходном представлении, что является обобщением информации\n",
    "\n",
    "- `Декодировщик` обратно преобразует  изображение в исходное состояние\n",
    "\n",
    "- `Функция потерь` сравнивает исходное изображение и полученное после декодирования"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Увеличение размерности</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/super.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Исходное` изображение в вымоком разрешении `уменьшается` в размерности\n",
    "- Изображение с `уменьшенной` размерность подается на `вход` сети\n",
    "- Изображение на `выходе` сравнивается с `исходным` изображением\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Методы оптимизации}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Градиентный спуск</a>\n",
    "\n",
    "<img src='img/gd.png'>\n",
    "\n",
    "- Изменение весов `всех объектов` на каждой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Стохастический градиентный спуск</a>\n",
    "\n",
    "<img src='img/sgd.png'>\n",
    "\n",
    "- Изменение весов `одного объекта` на каждой итерации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Стохастический градиентный спуск с импульсом</a>\n",
    "\n",
    "- Вдоль направления, на котором градиент меняет знак сумма градиентов будет уменьшаться\n",
    "- Вдоль направления, на котором градиент не меняет знак сумма градиентов будет увеличиаться\n",
    "<img src='img/moment0.png'>\n",
    "\n",
    "- $\\alpha$ : шаг обучения\n",
    "- $\\beta$ : коэффициент импульса\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>RMSprop</a>\n",
    "\n",
    "- Если производная по некоторому направлению `большая`, то RMSprop `замедляет` скорость обучения по этому направлению\n",
    "- Если производная по некоторому направлению `маленькая`, то RMSprop `ускоряет` скорость обучения по этому направлению\n",
    "\n",
    "<img src='img/rmsprop.png'>\n",
    "\n",
    "- $\\alpha$ : шаг обучения\n",
    "- $\\nabla L$ : градиент по всем весам \n",
    "- $EMA_\\beta^t(\\nabla L)$: экспоненциальное скользящее среднее от квадрата градиента функции потерь в текущий момент времени\n",
    "    - больший вес придается последним данным\n",
    "    - вес регулируется параметром $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Adam</a>\n",
    "\n",
    "- В отличие от RMSprop оптимизируется градиент экспоненциального скользящего  среднего (в числителе)\n",
    "\n",
    "<img src='img/adam.png'>\n",
    "\n",
    "Оптимальные параметры:\n",
    "- $\\alpha = 3 \\cdot 10^{-4}$ \n",
    "- $\\beta_1 = 0.9$ \n",
    "- $\\beta_2 = 0.999$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Сверточные нейронные сети}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Двумерная свертка</a>\n",
    "\n",
    "`Сумма` `поэлементного` `умножения` ядра на матрицу\n",
    "<img src='img/conv2d.png' width=500>\n",
    "- Stride = 1\n",
    "- Padding = 1\n",
    "___\n",
    "\n",
    "<img src='img/conv2d2.png' width=500>\n",
    "\n",
    "- Stride = 2\n",
    "- Padding = 1\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([[4, 2, -1],[-6, 0, 5],[3, 2, 2]])\n",
    "kernel = np.array([[0, 1, 2],[1, -1, 0],[1, 0, -2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pad = np.pad(a, pad_width=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_pad_1 = a_pad[:3, :3]\n",
    "a_pad_2 = a_pad[:3, 2:]\n",
    "a_pad_3 = a_pad[2:, :3]\n",
    "a_pad_4 = a_pad[2:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4,  3],\n",
       "       [-9,  5]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_2 = np.array(\n",
    "[\n",
    "[np.sum(a_pad_1 * kernel)],\n",
    "[np.sum(a_pad_2 * kernel)],\n",
    "[np.sum(a_pad_3 * kernel)],\n",
    "[np.sum(a_pad_4 * kernel)],\n",
    "    ]\n",
    "    )\n",
    "conv_2.reshape(2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Pytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-4.,  3.],\n",
       "          [-9.,  5.]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[4, 2, -1],[-6, 0, 5],[3, 2, 2]], dtype=torch.float32).reshape(1,1,3,3)\n",
    "kernel = torch.tensor([[0, 1, 2],[1, -1, 0],[1, 0, -2]], dtype=torch.float32).reshape(1,1,3,3)\n",
    "\n",
    "torch.conv2d(a, stride=2, padding=1, weight=kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Многомерная свертка</a>\n",
    "\n",
    "\n",
    "<img src='img/conv3d1.png' width=500>\n",
    "\n",
    "- Для каждого канал может применяться свое ядро\n",
    "___\n",
    "<img src='img/conv3d2.png' width=500>\n",
    "\n",
    "- $M$: кол-во каналов на выходе равно кол-ву прмененных ядер\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Пулинг</a>\n",
    "\n",
    "Позволяет экономить `память`\n",
    "\n",
    "<img src='img/pool.png' width=500>\n",
    "\n",
    "Виды пулинга:\n",
    "- MaxPool\n",
    "- AvgPool\n",
    "- MinPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.,  2.,  3.,  4.],\n",
      "         [ 5.,  6.,  7.,  8.],\n",
      "         [ 9., 10., 11., 12.],\n",
      "         [13., 14., 15., 16.]]])\n",
      "tensor([[[ 6.,  8.],\n",
      "         [14., 16.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(1, 17, dtype=torch.float32).reshape(1,4,4)\n",
    "max_pool = torch.max_pool2d(a, kernel_size=2)\n",
    "print(a)\n",
    "print(max_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Архитектуры сверточных нейронных сетей}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>LeNet</a>\n",
    "\n",
    "Создана для решения задачи `MNIST`\n",
    "\n",
    "- `Первый блок`\n",
    "\n",
    "<img src='img/lenet1.png' width=400>\n",
    "\n",
    "- `Архитектура LeNet`\n",
    "\n",
    "<img src='img/lenet2.png' width=700>\n",
    "\n",
    "- Функция активации `Tanh`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>AlexNet</a>\n",
    "\n",
    "Создана для решения задачи `ImageNet`\n",
    "\n",
    "- `Архитектура AlexNet`\n",
    "\n",
    "<img src='img/alexnet.png' width=700>\n",
    "\n",
    "- Функция активации `Relu`\n",
    "- Последние 2 светки - каскад сверток 3 $\\times$ 3 и 3 $\\times$ 3, заменяющий свертку 5 $\\times$ 5, но использующие меньше параметров:\n",
    "    - $5 \\cdot 5  + 1(bias) = 26$\n",
    "    - $3 \\cdot 3 + 3 \\cdot 3 + 2(bias) = 20$\n",
    "    - <img src='img/cascade.png' width=400></img>\n",
    "        - Слева область свертки 5 $\\times$ 5\n",
    "        - Справа голубая область свертки 3 $\\times$ 3 как результат предыдущей свертки 3 $\\times$ 3 (оранжевая)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>VGG</a>\n",
    "\n",
    "- `Архитектура AlexNet`\n",
    "\n",
    "<img src='img/vgg.png' width=700>\n",
    "\n",
    "- Используются `каскады сверток` для оптимизации количества параметров\n",
    "- В `процессе обучения` к исходной сети `A` добавляются слои:\n",
    "    - `B`: conv3-64, conv3-128\n",
    "    - `C`: conv1-256, conv1-512, conv1-512\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>GoogleNet</a>\n",
    "\n",
    "<img src='img/googlenet.png' width=900></img>\n",
    "\n",
    "- `Inseption block`:\n",
    "\n",
    "<img src='img/incept.png' width=500>\n",
    "\n",
    "- В блоке `параллельно` выполняются различные виды сверток и затем `конкатенируются`\n",
    "    - Свертка 1 $\\times$ 1 ( `bottle neck` ) пердобрабатывает входные каналы и уменьшает или увеличивает число каналов \n",
    "        - <img src='img/bottle.png' width=600></img>\n",
    "\n",
    "- Доболнительные выходы с `функциями потерь` помогают устранять `затухание` градиента\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>ResNet</a>\n",
    "\n",
    "Из-за большой глубины и Avg pooling вконце обучения,  ResNet `может принимать изображения любого размера` (кратные 32)\n",
    "\n",
    "<img src='img/resnet.png' width=900></img>\n",
    "\n",
    "- `Residual block`\n",
    "\n",
    "<img src='img/rb.png' width=600></img>\n",
    "\n",
    "- Т.к. результат выхода из Residual block - $\\boxed{f(x) + x }$, то\n",
    "    -  $(f(x) + x )'$ = $\\boxed{f'(x) + 1 }$,\n",
    "    -  что добавляет 1 к градиенту и устраняет затухание градиента при обратном распространении\n",
    "\n",
    "- В конце после сверточных слоев производится `Avg polling`, превращая выход в `вектор` с размером 1 $\\times$ C, где\n",
    "    - С: кол-во каналов\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>DenseNet</a>\n",
    "\n",
    "<img src='img/dense.png' width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>SENet</a>\n",
    "\n",
    "<img src='img/senet.png' width=900></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Регуляризация и нормализация}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Early stopping</a>\n",
    "\n",
    "Обучение модели необходимо `остановить`, когда ошибка на `проверочном` наборе данных перестаёт `уменьшаться`. Это может быть признаком того, что модель начинает запоминать шум в данных, а не обобщать их.\n",
    "\n",
    "<img src='img/estop.png' width=600></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Регуляризация Ridge</a>\n",
    "\n",
    "К функции потерь добавляется дополнительный штраф за `большие значения весов`, как `сумма квадратов весов`\n",
    "\n",
    "$\\boxed{ L = \\sum_{s=0}^s \\Big(\\sum_{i=1}^{n} w_i \\cdot x_i - y_s \\Big)^2 + \\lambda \\cdot \\sum_{j=1}^j w^2_j }$\n",
    "\n",
    "<img src='img/reg1.png' width=400></img>\n",
    "\n",
    "- Увеличение значения коэффициента регулеризации $\\lambda$ сглаживает пики аппроксимирующей функции\n",
    "\n",
    "Объяснение работы L2-регуляризации:\n",
    "\n",
    "<img src='img/ridge.png' width=400></img>\n",
    "\n",
    "- Слева линии уровня значения функции потерь для значений весов $w_1$ и $w_2$\n",
    "- Справа окружности - максимумы значений весов $w_1$ и $w_2$ при регуляризации, т.к. \n",
    "    - $w_1^2 + w_2^2 = R^2$ : уравнение окружности\n",
    "\n",
    "- Чем `больше` $\\lambda$, тем `больше` влияние регуляризации и `меньше` окружность\n",
    "    - Значения весов стягиваются к миниимальным значениям,\n",
    "    - Значение функции потерь увеличивается\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Регуляризация Lasso</a>\n",
    "\n",
    "К функции потерь добавляется дополнительный штраф за `большие значения весов`, как `сумма модулей весов`\n",
    "\n",
    "$\\boxed{ L = \\sum_{s=0}^s \\Big(\\sum_{i=1}^{n} w_i \\cdot x_i - y_s \\Big)^2 + \\lambda \\cdot \\sum_{j=1}^j | w_j | }$\n",
    "\n",
    "<img src='img/lasso.png' width=500></img>\n",
    "\n",
    "- Есть высокая вероятность `занулить` некоторые веса, т.к. `наибольшие` значения весов лежат на `осях`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>DropOut</a>\n",
    "\n",
    "На каждой итерации обучения случайно выбранные нейроны (или связи между ними) временно `исключаются` из процесса.  \n",
    "Это помогает предотвратить запоминание сетью ненужных деталей и повышает её обобщающую способность.\n",
    "\n",
    "\n",
    "Drop `connection`:\n",
    "\n",
    "<img src='img/doc.png' width=200></img>\n",
    "\n",
    "Drop `neuron`:\n",
    "\n",
    "<img src='img/don.png' width=200></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>Нормализация</a>\n",
    "\n",
    "$\\boxed{ x_{stand} = \\frac{ x_i - \\mu}{\\sigma} }$, где\n",
    "\n",
    "- $\\boxed{ \\mu = \\frac{ \\sum_{i=1}^n x_i }{n} }$: среднее значение\n",
    "\n",
    "- $\\boxed{ \\sigma = \\sqrt{\\frac{ \\sum_{i=1}^n (x_i - \\mu)^2 }{n - 1}} }$: стандартное отклонение\n",
    "\n",
    "\n",
    "Процесс приведения входных данных к определённому диапазону значений.  \n",
    "Нормализация позволяет:\n",
    "- улучшить производительность\n",
    "- улучшить стабильность модели\n",
    "- ускорить процесс обучения.\n",
    "\n",
    "\n",
    "`Центрирование` упрощает подбор весов, т.к. размещает данные ближе к 0 значениям\n",
    "\n",
    "<img src='img/cener.png' width=500></img>\n",
    "\n",
    "- Стандартизация позволяет весам признаков вносить одинаковый вклад в функцию потерь,\n",
    "    - Функция потерь принимает округлую форму вместо вытянутой, \n",
    "        - что упрощает поиск минимума при помощи градиентного спуска\n",
    "\n",
    "<img src='img/std.png' width=500></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### <a>BatchNorm</a>\n",
    "\n",
    "Нормализация данных после каждого слоя сети для каждого батча \n",
    "\n",
    "$\\boxed{ x_{stand}^{batch} = \\frac{ x_i - \\mu_{batch}}{\\sigma_{batch}} \\cdot \\gamma + \\beta }$, где\n",
    "\n",
    "- $\\gamma$ : обучаемый вектор размера входящих данных, вносящий смещение в стандартное отклонение\n",
    "- $\\beta$ : обучаемый вектор размера входящих данных, вносящий смещение в среднее значение\n",
    "\n",
    "На инференсе $\\mu$ и $\\sigma$ принимаются как значения скользящего экспоненциального среднего по $\\mu_{batch}$ и $\\sigma_{batch}$\n",
    "\n",
    "При обучении нельзя использовать значения скользящего экспоненциального среднего по $\\mu_{batch}$ и $\\sigma_{batch}$ т.к. проблемно высчитать производную на обратном ходе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000, -1.0000,  0.0000,  0.0000, -1.0000],\n",
       "        [ 0.0000,  1.0000,  0.0000,  0.0000,  1.0000]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def custom_batch_norm1d(input_tensor, eps):\n",
    "    normed_tensor = (input_tensor - input_tensor.mean(dim=0, keepdim=True)) / (torch.sqrt(input_tensor.var(dim=0, keepdim=True, unbiased=False)+ eps) )\n",
    "    return normed_tensor\n",
    "\n",
    "input_tensor = torch.Tensor([[0.0, 0, 1, 0, 2], [0, 1, 1, 0, 10]])\n",
    "batch_norm = nn.BatchNorm1d(input_tensor.shape[1], affine=False)\n",
    "\n",
    "custom_batch_norm1d(input_tensor, eps=1e-05) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Данные нормируются по `признакам`, а не по объектам\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## <center> <a>$\\boxed{\\text{Метод максимального правдоподобия}}$</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод максимального правдоподобия помогает получать `функции потерь` для данных с `различным распределением`\n",
    "\n",
    "$\\boxed{1}$ Вероятность `полного набора набдюдений`:\n",
    "- $\\boxed{ P_X = p_w(x_0) \\cdot p_w(x_1) ... p_w(x_i)}$, где\n",
    "    - $x_0$: `независимое` наблюдение с `одинаковой` функцией рапредления\n",
    "    - $w$: ненаблюдаемые параметры распределения\n",
    "\n",
    "$\\boxed{2}$ Для того, чтобы получить `максимальные значения вероятности` полного набора данных необходимо `подбирать параметры` $w$, при которых веротность имеет максимальное значение.\n",
    "\n",
    "- $\\boxed{ w = argmax (P_X)}$\n",
    "\n",
    "$\\boxed{3}$ `Максимизацию произведения`вероятностей можно упростить `максимизацией суммы`логарифмов вероятностей, т.к логарифм - монотонно возрастающая функция:\n",
    "\n",
    "- $\\boxed{ w = argmax (\\log P_X)}$:\n",
    "\n",
    "    - $\\boxed{ \\log P_X = \\log p_w(x_0) + \\log  p_w(x_1) +...+ \\log  p_w(x_i)}$\n",
    "\n",
    "$\\boxed{4}$ Если наблюдения из нормального распределения, то `вероятность отдельного наблюдения`:\n",
    "\n",
    "- $\\boxed{ p_w(x_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2} } \\cdot e^{-\\frac{(x_i - \\mu)^2}{2 \\sigma^2}}}$\n",
    "\n",
    "$\\boxed{5}$ Сумма логарифмов вероятностей наблюдений, которую нужно `максимизировать`, отбрасывая параметры `не влияющие на максимизацию`, сводится к:\n",
    "\n",
    "- $\\boxed{argmax (\\log P_X) =  -\\sum_{i=1}^N (x_i - \\mu)^2}$\n",
    "\n",
    "    - $\\boxed{argmin (\\log P_X) =  \\sum_{i=1}^N (x_i - \\mu)^2}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
